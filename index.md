# Data Science and Decision Making

### Overall

This is a new module on Data Science and Decision Making - we will examine most aspects of modern data science and try to create a fully-fledged end-to-end data science app. The module outline can be found [here](https://www.essex.ac.uk/modules/Default.aspx?coursecode=CE888&year=17) - but it is still subject to minor changes. The lecture notes and lab scripts for this course will be made available and further developed during the module.  

This is a very hands-on module, where the goal is to give you sufficient breadth and depth to work as an independent data scientist. The course is assessed solely through coursework. 



### Lectures
Lectures take place on **Tuesday 09:00-11:00 at Room 6.345**. 

<a id="lec1"></a> 

1. [Lecture 1: Introduction](./slides/01-Introduction-slides.pdf), [Handouts](./slides/01-Introduction-handouts.pdf) 

<a id="lec2"></a> 

2. [Lecture 2: Summary and resampling statistics](./slides/02-Stats-Slides.pdf), [Handouts](./slides/02-Stats-handouts.pdf) 

<a id="lec3"></a> 

3. [Lecture 3: Predictive modelling](./slides/03-Modelling-slides.pdf), [Handouts](./slides/03-Modelling-handouts.pdf) 

<a id="lec4"></a> 

4. [Lecture 4: Bandits](./slides/04-Bandits-slides.pdf), [Handouts](./slides/04-Bandits-handouts.pdf) 

<a id="lec5"></a> 

5. [Lecture 5: Recommender systems](./slides/05-Recommender-slides.pdf), [Handouts](./slides/05-Recommender-handouts.pdf) 

<a id="lec6"></a> 

6. [Lecture 6: Data exploration](./slides/06-Exploration-slides.pdf), [Handouts](./slides/06-Exploration-handouts.pdf) 

<a id="lec7"></a> 

7. [Lecture 7: Neural networks](./slides/07-Neural-slides.pdf), [Handouts](./slides/07-Neural-handouts.pdf) 


### Labs
Labs are every **Fridays 15:00-18:00, CES Lab 3**. Labs are assessed and they should be completed either in class or later on. 

Please download the lab Virtual Machine from here: [MLVM Virtual Machine](https://docs.google.com/uc?id=0B_kDfEzMuWD6ZGJFU1VfeEY3TnM&export=download)

All labs can be found here: [https://github.com/ssamot/ce888/tree/master/labs/](https://github.com/ssamot/ce888/tree/master/labs/)

<a id="lab1"></a>

1. [Lab 1: VM setup and simple emotion detection](https://github.com/ssamot/ce888/tree/master/labs/lab1) 

<a id="lab2"></a>

2. [Lab 2: Creating plots, overleaf and confidence bounds](https://github.com/ssamot/ce888/tree/master/labs/lab2) 

<a id="lab3"></a>

3. [Lab 3: Jupiter/IPython and Classification](https://github.com/ssamot/ce888/tree/master/labs/lab3) 

<a id="lab4"></a>

4. [Lab 4: Bandits and time series plotting](https://github.com/ssamot/ce888/tree/master/labs/lab4) 

<a id="lab5"></a>

5. [Lab 5: Recommender systems and jokes](https://github.com/ssamot/ce888/tree/master/labs/lab5) 

<a id="lab6"></a>

6. [Lab 6: Clustering and visualisation](https://github.com/ssamot/ce888/tree/master/labs/lab6) 

<a id="lab6"></a>

7. [Lab 6: Neural networks and MNIST](https://github.com/ssamot/ce888/tree/master/labs/lab7) 





### IDEs
You can use whatever IDE you want for the course, however my proposal would be to use PyCharm - which is installed in the lab machines:

*  [PyCharm](https://www.jetbrains.com/pycharm/)


### Assessment

The objective of the module and the main assignment is to produce a Data Science app (i.e. make use of data to generate results, make inferences, present the results to third parties) and write down the description of the methods and the results in a scientific paper. Some ideas for possible apps are provided [here](#assignment-suggestions).


* Assigments
	* [Assignment 1](./assignments/ce888-assignment-1.pdf)

	

See FASER for exact assignment deadlines. 


### Readings

Every lecture will come with a set of online reading suggestions - they will be added here. 

**Lecture 1**

[Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)

[Breiman, Leo. "Statistical modeling: The two cultures (with comments and a rejoinder by the author)." Statistical Science 16.3 (2001): 199-231.](http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726%20)

[Anderson, Philip W. "More is different." Science 177.4047 (1972): 393-396.](https://www.tkm.kit.edu/downloads/TKM1_2011_more_is_different_PWA.pdf)


**Lecture 2**

[Efron, Bradley, and Robert J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.](http://cds.cern.ch/record/526679/files/0412042312_TOC.pdf)

[Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. "False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant." Psychological science 22.11 (2011): 1359-1366.](http://www.haas.berkeley.edu/groups/online_marketing/facultyCV/papers/nelson_false-positive.pdf)

[Schenker, Nathaniel, and Jane F. Gentleman. "On judging the significance of differences by examining the overlap between confidence intervals." The American Statistician 55.3 (2001): 182-186.](htps://www.jstor.org/stable/2685796)

**Lecture 3** 

[Jake VanderPlas, Python Data Science Handbook Essential Tools for Working with Data 2016. O'Reilly Media, 2016](https://github.com/jakevdp/PythonDataScienceHandbook)

[Kohavi, Ron. "A study of cross-validation and bootstrap for accuracy estimation and model selection." IJCAI. Vol. 14. No. 2. 1995.](https://pdfs.semanticscholar.org/0be0/d781305750b37acb35fa187febd8db67bfcc.pdf)

[Geman, Stuart, Elie Bienenstock, and René Doursat. "Neural networks and the bias/variance dilemma." Neural computation 4.1 (1992): 1-58.](https://stuff.mit.edu/afs/athena.mit.edu/course/6/6.435/www/Geman92.pdf)

[scikit-learn's website](http://scikit-learn.org/)

**Lecture 4**

[White, John. Bandit algorithms for website optimization. " O'Reilly Media, Inc.", 2012.](http://shop.oreilly.com/product/0636920027393.do)

[Oza, Nikunj C. "Online bagging and boosting." Systems, man and cybernetics, 2005 IEEE international conference on. Vol. 3. IEEE, 2005.](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20050239012.pdf)

[Osband, Ian, et al. "Deep exploration via bootstrapped DQN." Advances In Neural Information Processing Systems. 2016.](http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn.pdf)

**Lecture 5**

[Simon Funk, Netflix Update: Try This at Home](http://sifter.org/~simon/journal/20061211.html)

[Barkan, Oren, and Noam Koenigstein. "Item2vec: neural item embedding for collaborative filtering." Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on. IEEE, 2016.](https://arxiv.org/pdf/1603.04259.pdf)

[Hu, Yifan, Yehuda Koren, and Chris Volinsky. "Collaborative filtering for implicit feedback datasets." Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on. Ieee, 2008.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&rep=rep1&type=pdf)

**Lecture 6**

[Rousseeuw, Peter J. "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." Journal of computational and applied mathematics 20 (1987): 53-65.](http://www.sciencedirect.com/science/article/pii/0377042787901257)

[Arthur, David, and Sergei Vassilvitskii. "k-means++: The advantages of careful seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2007.](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)

**Lecture 7**

[Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the dimensionality of data with neural networks." science 313.5786 (2006): 504-507.](https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf)

[Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, 2016.](http://www.deeplearningbook.org/)

[Bengio, Yoshua. "Learning deep architectures for AI." Foundations and trends® in Machine Learning 2.1 (2009): 1-127.](http://www.nowpublishers.com/article/DownloadSummary/MAL-006)


### People
* Module Supervisor: *Spyros Samothrakis*, <ssamot@essex.ac.uk>

* * * 
* * * 




